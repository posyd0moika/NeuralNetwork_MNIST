{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Input, Flatten\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mnist\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x1_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "y1_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential() # Создаём модель\n",
    "model.add(Flatten(input_shape=(28, 28, 1))) # Добавляем слой, который будет принимать изображение (28,28,1), после чего он это изображение превращает в вектор (784,)\n",
    "model.add(Dense(64, activation=\"relu\", name=\"hidden_1\")) # первый полносвязный слой,\n",
    "# где 64 нейрона и функция активации \"relu\", тут можете экспериментировать с этими двумя показателями,\n",
    "# также доавлять новые слои\n",
    "model.add(Dense(10, activation=\"softmax\", name=\"output\")) # Выходной слой (обязательно), 10 выходов ,потому что , у нас 10 цифр от 0 до 9, функция активации \"softmax\" - это самая оптимальная функция для последнего слоя, можно еще попробовать \"sigmoid\", но могу возникнуть проблемы с этой функцией в дальнейшем\n",
    "\n",
    "model.summary() #  выводим строение нашей сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy', # Выбираю ошибку\n",
    "    metrics=['accuracy'], # Метрика оценки работы сети\n",
    "    optimizer=\"adam\" # Выбираю оптимизатор для модели\n",
    ")\n",
    "his = model.fit(\n",
    "    x1_train,  # Подаём на вход данные, которые подаются в НС\n",
    "    y1_train,  # Данные которые мы требуем на выходе нейронной сети,\n",
    "    epochs=20, # Количество эпох\n",
    "    validation_split=0.2,  # validation_split=0.2 - значит, что 20% тренировочной выборки будет использоваться для\n",
    "    # проверки работоспособности сети на данных которых она не видела(не тренировалась, просто для статистики)\n",
    "    batch_size=32 #  Размер батча, т.е. сколько изображений мы подаём на вход, чтобы расчитать по ним градиент, это делается для того, чтобы более эффективно находить оптимальное решение\n",
    ") # fit - это метод для обучения модели,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Model_ex_256\", save_format=\"h5\") # сохраняю модель в диск в формате h5 - формат необязательно указывать,\n",
    "# но в рамках данного урока, лучше сохранять именно в таком формате, для чего узнаем в будущем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train, to_categorical(y_train, 10)) # Проверяем модель на тестовой выборке\n",
    "# 1) loss 2) accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # Импортируем модуль для вывода графиков\n",
    "\n",
    "h = his.history\n",
    "plt.plot(h[\"loss\"]) # синяя линия - ошибка на тренировочной выборке\n",
    "plt.plot(h[\"val_loss\"]) # оранжевая линия - это ошибка на валидационной выборке\n",
    "plt.show() # Выводим график"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h['accuracy'])  # статистика по работе сети на тренировочной выборке синий цвет\n",
    "plt.plot(h['val_accuracy']) # статистика по работе сети на валидационной выборке оранжевый цвет\n",
    "plt.show()\n",
    "# на моём графике отчетливо видно, что после 3-7 эпохи расхождение между графиками все большое и больше - это значит,\n",
    "# что нейронная сеть начала переобучаться, чтобы это исправить можно понизить количество нейронов на слоях(кроме последнего),\n",
    "# так же можно использовать регулизаторы(L1,L2,Dropout). Советую попробовать Dropout*, синтаксис такой <<< Dropout(0.3)\n",
    "# - 0.3 значит будет на предыдущем слое будет случайно отброшено 30% нейронов, что позволит сети пользоваться\n",
    "# частичным описанием признаков, тем самым она сможет использовать свои нейроны более эффективно.\n",
    "# Прочитать подробней про него можно тут https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\n",
    "# этот материал со *, т.к. является дополнительный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Этот материал необязательный, дело в том, что наша база данных устроена так, что цифры расположены посередине, из-за чего наша модель способна распознавать,\n",
    "model2 = keras.Sequential([ # Еще один способ как можно создавать модель\n",
    "    Input((28,28,1)),  # так можно задавать размерность входных данных\n",
    "    Flatten(), # Превращает одноканальное изображение в вектор\n",
    "    Dense(64, activation=\"relu\", name=\"hidden_1\"),\n",
    "    Dense(10, activation=\"softmax\", name=\"output\")\n",
    "])\n",
    "model2.summary()\n",
    "\n",
    "import extensionbase # Импортируем модуль, который расширяет, нашу базу данных в 77 раз\n",
    "\n",
    "sd = 100 # sd - size drop - чем меньше этот параметр, тем лучше, но кушает много оперативки, оптимально 100. Рассчитывается так 60_000 * 77 // sd такими кусочками вам будут подаваться данные\n",
    "gen = extensionbase.gen_extension_base(x_train, y_train, size_drop=sd)\n",
    "\n",
    "av = []\n",
    "ah = []\n",
    "\n",
    "model2.compile(optimizer=\"adam\",  # Выбираю оптимизатор для модели\n",
    "               loss='categorical_crossentropy',  # Выбираю ошибку\n",
    "               metrics=['accuracy'])  # Метрика оценки работы сети\n",
    "\n",
    "epochs = 2\n",
    "for i in range(epochs):\n",
    "    for j in range(sd):\n",
    "        x, y = next(gen)  # просим генератор нам выдать значения для обучения\n",
    "        # обращаю внимание, что генератору мы подаём не преобразованные данные, они внутри генератора преобразуются\n",
    "        h = model2.fit(x, y, validation_split=0.25, shuffle=True,batch_size=77).history\n",
    "        # добавляем в наши списки результаты работы сети\n",
    "        av.append(h[\"val_accuracy\"])\n",
    "        ah.append(h[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(\"Model ex2 64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выводим графики\n",
    "plt.plot(av)\n",
    "plt.plot(ah,c=\"r\") # красный - это тренировочная выборка, синий - это валидационная\n",
    "plt.show()\n",
    "\n",
    "# обратим внимание, что у нас бросает статистические показатели то вверх, то вниз,\n",
    "# это связано со сложностью определённых цифр, некоторые цифры сложно отличить даже человеку.\n",
    "# алгоритм увеличения стандартной базы данных устроен, так что со второй эпохе данного алгоритма, мы с определенной\n",
    "# вероятностью будем натыкаться в тренировочной выборке с теми данными, которые были в валидационной,на 1 эпохе."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
